{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " 연습문제 : 네이버 카페의 게시물과 댓글 정보 수집하기\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# selenium 예제 15: # 등업을 해야한다  #게시판이름 메모장에 복사하기 \n",
    "# 네이버에 로그인 후\n",
    "# 네이버 카페 주소 입력받고\n",
    "# 게시판 이름 입력 받고\n",
    "#원하는 건수를 입력받아서\n",
    "#각 페이지에 있는 블로그의 상세 텍스트를 모두 수집한 후\n",
    "#현재 날짜와 시간으로 폴더를 만들고\n",
    "#그 폴더아래 현재 날짜와 시간으로 파일명을 만들어 저장하기\n",
    "\n",
    "# 크롬일 경우 네이버 로그인 한 후 브라우저 하단의 플래시 경고창을 반드시 닫고 시작해야 합니다! \n",
    "\n",
    "#참고 사이트 : http://yahwang.tk/posts/4\n",
    "\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import numpy  # pip install numpy 를 해야 사용가능함\n",
    "import pandas as pd  # pip install pandas 해야 사용가능하고 numpy 가 먼저 설치되어 있어야\n",
    "import xlwt # excel 형태로 저장하기 위해 필요함. pip install xlwt 해야 사용가능함\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "#Step 2. 사용자에게 검색어 키워드를 입력 받습니다.\n",
    "print(\"=\" *80)\n",
    "print(\" 연습문제 : 네이버 카페의 게시물과 댓글 정보 수집하기\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "# Step 3. 네이버에 로그인 합니다\n",
    "\n",
    "s_time = time.time( )\n",
    "\n",
    "path = \"c:/temp/chromedriver_240/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(\"https://www.naver.com\")\n",
    "\n",
    "time.sleep(20)  ## 사용자가 로그인 할 때까지 20초 기다립니다\n",
    "\n",
    "cafe_name = input('1.크롤링할 카페 이름을 입력하세요: ')\n",
    "cafe_url = 'http://cafe.naver.com/'+ cafe_name\n",
    "    \n",
    "gesipan_name = input('2.조회할 게시판의 이름을 입력하세요(기본값:전체글보기): ')\n",
    "if gesipan_name == '':\n",
    "        gesipan_name = '전체글보기'\n",
    "    \n",
    "print(\"=\" *80)\n",
    "choice = int(input('''\n",
    "[조회 방법 선택하기]\n",
    "    \n",
    "1.특정 키워드와 조회기간 입력하여 조회하기\n",
    "2.오늘을 기준으로 전체 건수만 입력하여 조회하기 \n",
    "3.특정 게시물을 지정하여 내용과 댓글 정보 수집하기\n",
    "    \n",
    "위 작업 중 원하는 작업을 선택하세요:  '''))\n",
    "\n",
    "print(\"=\" *80)\n",
    "print(\"\\n\")\n",
    "\n",
    "    \n",
    "if choice == 1 :\n",
    "        start_date=input('1.조회 시작 날짜를 입력하세요(기본값:2018-01-01): ')\n",
    "        if start_date == '' :\n",
    "            start_date = '2018-01-01'\n",
    "            \n",
    "        end_date=input('2.조회 종료 날짜를 입력하세요(기본값:2018-12-31): ')\n",
    "        if end_date == '' :\n",
    "            end_date = '2018-12-31'\n",
    "\n",
    "        query_txt=''\n",
    "        while ( query_txt==''):\n",
    "            query_txt = input('3.검색할 키워드를 반드시 입력하세요: ')\n",
    "    \n",
    "        try :\n",
    "            cnt = int(input('4.크롤링 할 건수는 몇건입니까?(기본건수:15건): '))\n",
    "        except ValueError :\n",
    "            cnt = 15\n",
    "\n",
    "        page_cnt = math.ceil(cnt / 15)  # 주어진 검색 건수를 크롤링 하기 위한 전체 페이지 번호\n",
    "    \n",
    "        f_dir = \"c:\\\\temp\\\\\"\n",
    "        #f_dir=input('4.파일이 저장될 경로만 쓰세요(예: c:\\\\temp\\\\ ) : ')\n",
    "        if f_dir == '' :\n",
    "            f_dir = \"c:\\\\temp\\\\\"\n",
    "        \n",
    "\n",
    "        print(\"=\" *80)\n",
    "        print(\"입력하신 정보로 총 %s 페이지의 데이터 수집을 시작합니다================================\" %page_cnt)\n",
    "        print(\"\\n\")\n",
    "  \n",
    "        driver.get(cafe_url)    \n",
    "\n",
    "        # 페이지 번호 만들기 - 10 페이지가 지나면 11 페이지가 없고 다음 버튼이 나오기 때문에\n",
    "        # 10 페이지 후에 11 페이지 대신 다음 페이지 누르려고 만든 변수\n",
    "\n",
    "        page_no=[]\n",
    "\n",
    "        for i in range(10,cnt) :\n",
    "            if i % 10 == 0 :\n",
    "                #page_no.append(i+1)\n",
    "                page_no.append(i+1)\n",
    "        #print(page_no)\n",
    "        \n",
    "\n",
    "        # Step 3. 카페 페이지를 엽니다\n",
    "        driver.get(cafe_url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        now = time.localtime()\n",
    "        s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "        os.chdir(f_dir)\n",
    "        os.makedirs(f_dir+s+'-'+cafe_name)\n",
    "        os.chdir(f_dir+s+'-'+cafe_name)\n",
    "        f_name=s+'-'+cafe_name+'.txt'\n",
    "        ff_name=f_dir+s+'-'+cafe_name+'\\\\'+s+'-'+cafe_name+'.txt'\n",
    "        fx_name=f_dir+s+'-'+cafe_name+'\\\\'+s+'-'+cafe_name+'.xls'\n",
    "        fc_name=f_dir+s+'-'+cafe_name+'\\\\'+s+'-'+cafe_name+'.csv'\n",
    "\n",
    "        def scroll_down(driver):\n",
    "          driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "          time.sleep(1)\n",
    "\n",
    "\n",
    "        i = 1\n",
    "        while (i <= 2):\n",
    "              scroll_down(driver) \n",
    "              i += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 플래시 경고창을 닫기\n",
    "        #driver.find_element_by_xpath(\"\"\"/html/body/div[2]/span[3]/button/img\"\"\").click( )\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        #선택한 게시판으로 이동하기\n",
    "        driver.find_element_by_link_text(\"%s\" %gesipan_name).click()   #여행후기\n",
    "        #driver.find_element_by_xpath(\"\"\"//*[@id=\"menuLink0\"]\"\"\").click( )   # 전체글보기 메뉴 선택\n",
    "        time.sleep(2) \n",
    "\n",
    "        \n",
    "\n",
    "        #Step 7. 검색 조건과 기간 버튼 클릭\n",
    "        driver.switch_to.frame('cafe_main')\n",
    "        #driver.switch_to.frame('wordchk')\n",
    "\n",
    "        driver.find_element_by_id(\"currentSearchDate\").click( )\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 직접 입력 시작 날짜 클릭\n",
    "        s_date = driver.find_element_by_xpath(\"\"\"//*[@id=\"input_1\"]\"\"\")     \n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"input_1\"]\"\"\").click()\n",
    "        s_date.clear()\n",
    "        time.sleep(1)\n",
    "\n",
    "        for c in start_date:\n",
    "            s_date.send_keys(c)\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        e_date = driver.find_element_by_xpath(\"\"\"//*[@id=\"input_2\"]\"\"\")\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"input_2\"]\"\"\").click()\n",
    "        e_date.clear()\n",
    "        time.sleep(1)\n",
    "\n",
    "        for c in end_date:\n",
    "            e_date.send_keys(c)\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        driver.find_element_by_id(\"\"\"btn_set\"\"\").click()\n",
    "\n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_name(\"query\").send_keys(query_txt)\n",
    "\n",
    "        # 검색버튼 클릭\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"main-area\"]/div[7]/form/div[3]/button\"\"\").click() # 여행후기 게시판 검색버튼\n",
    "        # driver.find_element_by_xpath(\"\"\"//*[@id=\"main-area\"]/div[7]/form/div[3]/button\"\"\").click() #전체글보기 검색버튼\n",
    "        time.sleep(2)    \n",
    "\n",
    "   \n",
    "\n",
    "        # 검색 결과 페이지의 상세 내역을 추출합니다\n",
    "\n",
    "        #원본 게시물의 URL 주소를 저장할 변수\n",
    "        article_urls_2=[]\n",
    "\n",
    "        click_count= 1       # 페이지 번호 넘길 때 사용할 변수\n",
    "        current_page = 1\n",
    "        count = 1\n",
    "\n",
    "        item_count = 0   # 총 수집한 게시물의 건수 저장 변수\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # 현재 페이지 - 1 페이지의 게시물의 원본 URL 목록 만들기\n",
    "        try : \n",
    "            all_list = soup.find('div','article-board result-board m-tcol-c')\n",
    "        except :\n",
    "            print(\"검색된 게시물이 없습니다\")\n",
    "        else :\n",
    "            a_list = all_list.find('table').find('tbody').find_all('tr')\n",
    "        \n",
    "\n",
    "        for i in a_list :\n",
    "            try :\n",
    "                a =i.find('div','inner_list').find('a')\n",
    "            except :\n",
    "                #print(\"현재 페이지에서 에러가 발생했습니다\")\n",
    "                continue\n",
    "            else :\n",
    "                a_url = a['href']\n",
    "                #print(a_url)\n",
    "                article_urls_2.append(a_url)\n",
    "                item_count += 1\n",
    "                      \n",
    "            if item_count >= cnt :\n",
    "                break\n",
    "                \n",
    "        print(\"현재 %s 페이지에서 %s 건의 정보를 추출했습니다^^\" %(click_count,len(article_urls_2)))\n",
    "   \n",
    "        click_count += 1     # 페이지 번호 1 증가\n",
    "              \n",
    "        # 2 페이지로 넘어가야 할 경우 아래 코드 실행\n",
    "        if cnt > 15:\n",
    "        \n",
    "            driver.find_element_by_link_text(\"\"\"%s\"\"\" %click_count).click()  # 2페이지 클릭\n",
    "        \n",
    "            while ( click_count <= page_cnt ) :\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "                #driver.switch_to.frame('cafe_main')\n",
    "                \n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                  \n",
    "                b = soup.find('div','article-board result-board m-tcol-c')\n",
    "                b_list = b.find('table').find('tbody').find_all('tr')                      \n",
    "               \n",
    "\n",
    "                for j in b_list :\n",
    "                    try :\n",
    "                        a =j.find('div','inner_list').find('a')                        \n",
    "                    except AttributeError:\n",
    "                        #print(\"현재 페이지에서 에러가 발생했습니다\")\n",
    "                        continue\n",
    "                    else :    \n",
    "                        a_url = a['href']\n",
    "                        #print(a_url)\n",
    "                        article_urls_2.append(a_url)\n",
    "                        item_count += 1\n",
    "                      \n",
    "                    if item_count >= cnt :\n",
    "                        break\n",
    "                    \n",
    "                print(\"현재 %s 페이지까지 총 %s 건의 URL 정보를 수집했습니다~~~^^\" %(click_count,len(article_urls_2)))\n",
    "\n",
    "             # 11 페이지 번호가 화면에 없기 때문에 > 기호를 누르게 코드를 작성합니다\n",
    "                i = math.floor(current_page/10)\n",
    "                click_count += 1 \n",
    "            \n",
    "                try :\n",
    "                    if click_count == 1000 :\n",
    "                         click_count2 = \"{:,}\".format(click_count)\n",
    "                         driver.find_element_by_link_text(\"\"\"%s\"\"\" %click_count2).click()\n",
    "                         break\n",
    "                    elif click_count == page_no[i] :                      \n",
    "                        driver.find_element_by_link_text(\"\"\"다음\"\"\").click()\n",
    "                    elif click_count > 1000 :\n",
    "                        print(\"최대 1000 페이지까지만 수집할 수 있습니다\")\n",
    "                        break\n",
    "                    else :\n",
    "                        driver.find_element_by_link_text(\"\"\"%s\"\"\" %click_count).click()\n",
    "                   \n",
    "                    current_page += 1\n",
    "                except :\n",
    "                    print(\"총 %s 페이지까지 URL 정보 수집을 완료했습니다\" %click_count)\n",
    "                    break\n",
    "            \n",
    "                time.sleep(random.randrange(1,30))\n",
    "                            \n",
    "           ## 실제 게시글의 내용 추출하기\n",
    "\n",
    "        if cnt > len(article_urls_2) :\n",
    "            cnt = len(article_urls_2)\n",
    "\n",
    "        print(\"전체 검색 결과 건수 :\",len(article_urls_2),\"건\")\n",
    "        print(\"실제 최종 출력 건수\",cnt)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"크롤링 할 총 페이지 번호: \", click_count-1)\n",
    "        print(\"=\" *80)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(\"총 %s 건의 데이터를 추출하겠습니다~~~~~~~\" %len(article_urls_2))\n",
    "\n",
    "        no = 1        # 데이터 건수 세기 \n",
    "    \n",
    "        no2=[]           # 목록 번호 저장 변수\n",
    "        write_no2=[]     # 게시글번호 저장 변수\n",
    "        url_2=[]         # 원본 URL 저장 변수\n",
    "        writer_id2=[]    # 작성자 이름 저장 변수\n",
    "        title2=[]        # 제목 저장 변수\n",
    "        wdate2=[]        # 작성 일자 저장 변수\n",
    "        content2=[]      # 내용 저장 변수\n",
    "        reple_cnt2=[]   # 리플 갯수 저장 변수\n",
    "        view_cnt2=[]\n",
    "\n",
    "        # 결과를 화면에 출력하기\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"게시물의 수집을 시작합니다~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\" )\n",
    "        print(\"\\n\")\n",
    "        count = 1\n",
    "    \n",
    "        no=[]           # 목록 번호 저장 변수\n",
    "        write_no=[]     # 게시글 번호 저장 변수\n",
    "        writer_id=[]    # 작성자 이름 저장 변수\n",
    "        title=[]        # 제목 저장 변수\n",
    "        wdate=[]        # 작성 일자 저장 변수\n",
    "        content=[]      # 내용 저장 변수\n",
    "        reple_cnt=[]    # 리플 갯수 저장 변수\n",
    "        view_cnt=[]     # 조회수 저장변수\n",
    "\n",
    "        for article in article_urls_2:\n",
    "       \n",
    "            full_url = cafe_url+'/'+article\n",
    "            driver.get(full_url)\n",
    "        \n",
    "            no2.append(no)            # 1.글번호 컬럼 내용 추가\n",
    "            url_2.append(full_url)    #2. 원본 URL 주소 컬럼 내용 추가\n",
    "        \n",
    "            try :\n",
    "                driver.switch_to.frame('cafe_main')\n",
    "            except  :\n",
    "                continue\n",
    "            else :    \n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "     \n",
    "                f = open(ff_name, 'a',encoding='UTF-8')\n",
    "            \n",
    "                all_content = soup.find('div','inbox')\n",
    "                #print(all_content)\n",
    "            \n",
    "                print(\"%s번째 게시물의 수집을 시작합니다~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\" %count )\n",
    "            \n",
    "                #1.게시글번호 추출 - write_no 변수\n",
    "                w_no_1 = all_content.find('div','etc-box').find('div','fr')\n",
    "                w_no_2 = w_no_1.select('table > tbody > tr > td > span > a')\n",
    "                w_no_3 = w_no_2[0]['href'].split('/')\n",
    "                write_no = w_no_3[4]\n",
    "                write_no2.append(write_no)\n",
    "            \n",
    "                f.write('------------------------------------------------------'+\"\\n\")\n",
    "                f.write(\"%s번째 게시물의 수집을 시작합니다~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\" %count + \"\\n\" )\n",
    "                f.write('1.게시글번호:'+ write_no + \"\\n\")\n",
    "            \n",
    "\n",
    "               #2.게시글에서 작성자 추출 - writer_id 변수\n",
    "                w_name_1 = all_content.find('div','etc-box').find('div','fl')\n",
    "                w_name_2 = w_name_1.select('table > tbody > tr > td > table > tbody > tr > td ')\n",
    "                writer_id = w_name_2[1].get_text()\n",
    "                #print(writer_name)\n",
    "                writer_id2.append(writer_id)\n",
    "                f.write('2.작성자닉네임:'+ writer_id + \"\\n\")\n",
    "            \n",
    "\n",
    "                #3.게시글에서 제목 추출 - title 변수\n",
    "            \n",
    "                title_1 = all_content.find('div','tit-box').find('div','fl')\n",
    "                title_2 = title_1.select('table > tbody > tr > td > span ')\n",
    "                title = title_2[0].get_text().replace(\"\\n\",\"\").strip()\n",
    "                #print(title)\n",
    "                title2.append(title)\n",
    "                f.write('3.게시글제목:'+ title + \"\\n\")\n",
    "                     \n",
    "\n",
    "                #4. 게시글에서 작성일자 추출 - wdate 변수\n",
    "                wdate_1 = all_content.find('div','tit-box').find('div','fr')\n",
    "                wdate_2 = wdate_1.select('table > tbody > tr > td ')\n",
    "                wdate = wdate_2[1].get_text().replace(\"\\n\",\"\").strip()\n",
    "                #print(wdate)\n",
    "                wdate2.append(wdate)\n",
    "                f.write('4.작성일자:'+ wdate + \"\\n\")\n",
    "            \n",
    "\n",
    "                #5. 본문 내용을 출력합니다 - content 변수 사용\n",
    "    \n",
    "                content_1 = all_content.select('#tbody')\n",
    "                content_2 = content_1[0].get_text().replace(\"\\n\",\"\").strip()\n",
    "                print(content_2)\n",
    "                content2.append(content_2)\n",
    "                f.write('5.본문내용:'+ content_2 + \"\\n\")\n",
    "\n",
    "              # 6.게시글에서 댓글수 추출 - reple_cnt 변수사용\n",
    "    \n",
    "                all_reple = soup.select('#cmtMenu')\n",
    "                reple_1 = all_reple[0].select('div > table > tbody > tr > td > a')\n",
    "                reple_2 = reple_1[0].get_text()\n",
    "                reple_3 = reple_2.replace(\",\",\"\")\n",
    "                reple_4 = re.search(\"\\d+\",reple_3)\n",
    "                try :\n",
    "                    reple_cnt = int(reple_4.group())\n",
    "                except :\n",
    "                    reple_cnt = 0\n",
    "                #print(reple_cnt)   \n",
    "                reple_cnt2.append(reple_cnt)\n",
    "                f.write('6.댓글수:'+ str(reple_cnt) + \"\\n\")\n",
    "\n",
    "                # 7.게시글에서 조회수  추출 - view_cnt 변수사용\n",
    "    \n",
    "                view_1 = all_reple[0].select('div > table > tbody > tr')\n",
    "                view_2 = view_1[0].find_all('td')\n",
    "                view_3 = view_2[4].get_text()\n",
    "                view_4 = view_3.replace(\",\",\"\")\n",
    "                view_5 = re.search(\"\\d+\",view_4)\n",
    "                try :\n",
    "                    view_cnt = int(view_5.group())\n",
    "                except :\n",
    "                    view_cnt = 0\n",
    "                #print(view_cnt)   \n",
    "                view_cnt2.append(view_cnt)\n",
    "                f.write('7.조회수:'+ str(view_cnt) + \"\\n\")\n",
    "                time.sleep(random.randrange(1,30)) \n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "                print(\"\\n\")\n",
    "\n",
    "                count += 1\n",
    "\n",
    "elif choice == 2 :      # 현재 날짜 기준으로 건수를 입력받아서 수집하는 부분\n",
    "        try :\n",
    "            cnt = int(input('1.크롤링 할 건수는 몇건입니까?(기본값:15): '))\n",
    "        except ValueError :\n",
    "            cnt = 15\n",
    "        #cnt = 2500\n",
    "        page_cnt = math.ceil(cnt / 15)  # 주어진 검색 건수를 크롤링 하기 위한 전체 페이지 번호\n",
    "    \n",
    "        f_dir = \"c:\\\\temp\\\\\"\n",
    "        #f_dir=input('4.파일이 저장될 경로만 쓰세요(예: c:\\\\temp\\\\ ) : ')\n",
    "        if f_dir == '' :\n",
    "            f_dir = \"c:\\\\temp\\\\\"\n",
    "\n",
    "        print(\"=\" *80)\n",
    "        print(\"입력하신 정보로 총 %s 페이지의 데이터 수집을 시작합니다============================\" %page_cnt)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # 페이지 번호 만들기 - 10 페이지가 지나면 11 페이지가 없고 다음 버튼이 나오기 때문에\n",
    "        # 10 페이지 후에 11 페이지 대신 다음 페이지 누르려고 만든 변수\n",
    "\n",
    "        page_no=[]\n",
    "\n",
    "        for i in range(10,cnt) :\n",
    "            if i % 10 == 0 :\n",
    "                #page_no.append(i+1)\n",
    "                page_no.append(i+1)\n",
    "\n",
    "        # Step 3. 카페 페이지를 엽니다\n",
    "        driver.get(cafe_url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        now = time.localtime()\n",
    "        s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "        os.chdir(f_dir)\n",
    "        os.makedirs(f_dir+s+'-'+cafe_name)\n",
    "        os.chdir(f_dir+s+'-'+cafe_name)\n",
    "        f_name=s+'-'+cafe_name+'.txt'\n",
    "        ff_name=f_dir+s+'-'+cafe_name+'\\\\'+s+'-'+cafe_name+'.txt'\n",
    "        fx_name=f_dir+s+'-'+cafe_name+'\\\\'+s+'-'+cafe_name+'.xls'\n",
    "        fc_name=f_dir+s+'-'+cafe_name+'\\\\'+s+'-'+cafe_name+'.csv'\n",
    "\n",
    "        def scroll_down(driver):\n",
    "          driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "          time.sleep(1)\n",
    "\n",
    "\n",
    "        i = 1\n",
    "        while (i <= 2):\n",
    "              scroll_down(driver) \n",
    "              i += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 플래시 경고창을 닫기\n",
    "        #driver.find_element_by_xpath(\"\"\"/html/body/div[2]/span[3]/button/img\"\"\").click( )\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        #선택한 게시판으로 이동하기\n",
    "        driver.find_element_by_link_text(\"%s\" %gesipan_name).click()   # 여행후기\n",
    "        #driver.find_element_by_xpath(\"\"\"//*[@id=\"menuLink0\"]\"\"\").click( )   # 전체글보기 메뉴 선택\n",
    "      \n",
    "        time.sleep(2) \n",
    "       \n",
    "\n",
    "        # 검색 결과 페이지에서 원본 페이지의 URL 주소를 먼저 추출합니다        \n",
    "        driver.switch_to.frame('cafe_main')\n",
    "        \n",
    "        #원본 게시물의 URL 주소를 저장할 변수\n",
    "        article_urls_2=[]\n",
    "\n",
    "        click_count= 1       # 페이지 번호 넘길 때 사용할 변수\n",
    "        current_page = 1\n",
    "        count = 1\n",
    "\n",
    "        item_count = 0   # 총 수집한 게시물의 건수 저장 변수\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # 현재 페이지 - 1 페이지의 게시물의 원본 URL 목록 만들기\n",
    "        all_list = soup.find_all('div','article-board m-tcol-c')\n",
    "        a_list = all_list[1].find('table').find('tbody').find_all('tr')\n",
    "        \n",
    "\n",
    "        for i in a_list :\n",
    "            try :\n",
    "                a =i.find('div','inner_list').find('a')\n",
    "            except :\n",
    "                #print(\"현재 페이지에서 에러가 발생했습니다\")\n",
    "                continue\n",
    "            else :\n",
    "                a_url = a['href']\n",
    "                #print(a_url)\n",
    "                article_urls_2.append(a_url)\n",
    "                item_count += 1\n",
    "                      \n",
    "            if item_count >= cnt :\n",
    "                break\n",
    "                \n",
    "        print(\"현재 %s 페이지에서 %s 건의 정보를 추출했습니다^^\" %(click_count,len(article_urls_2)))\n",
    "   \n",
    "        click_count += 1     # 페이지 번호 1 증가\n",
    "              \n",
    "        # 2 페이지로 넘어가야 할 경우 아래 코드 실행\n",
    "        if cnt > 15:\n",
    "        \n",
    "            driver.find_element_by_link_text(\"\"\"%s\"\"\" %click_count).click()  # 2페이지 클릭\n",
    "        \n",
    "            while ( click_count <= page_cnt ) :\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "                #driver.switch_to.frame('cafe_main')\n",
    "                \n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                  \n",
    "                b = soup.find_all('div','article-board m-tcol-c')\n",
    "                b_list = b[1].find('table').find('tbody').find_all('tr')                       \n",
    "               \n",
    "\n",
    "                for j in b_list :\n",
    "                    try :\n",
    "                        a =j.find('div','inner_list').find('a')                        \n",
    "                    except AttributeError:\n",
    "                        #print(\"현재 페이지에서 에러가 발생했습니다\")\n",
    "                        continue\n",
    "                    else :    \n",
    "                        a_url = a['href']\n",
    "                        #print(a_url)\n",
    "                        article_urls_2.append(a_url)\n",
    "                        item_count += 1\n",
    "                      \n",
    "                    if item_count >= cnt :\n",
    "                        break\n",
    "                    \n",
    "                print(\"현재 %s 페이지까지 총 %s 건의 URL 정보를 수집했습니다~~~^^\" %(click_count,len(article_urls_2)))\n",
    "\n",
    "             # 11 페이지 번호가 화면에 없기 때문에 > 기호를 누르게 코드를 작성합니다\n",
    "                i = math.floor(current_page/10)\n",
    "                click_count += 1 \n",
    "            \n",
    "                try :\n",
    "                    if click_count == 1000 :\n",
    "                         click_count2 = \"{:,}\".format(click_count)\n",
    "                         driver.find_element_by_link_text(\"\"\"%s\"\"\" %click_count2).click()\n",
    "                         break\n",
    "                    elif click_count == page_no[i] :                      \n",
    "                        driver.find_element_by_link_text(\"\"\"다음\"\"\").click()\n",
    "                    elif click_count > 1000 :\n",
    "                        print(\"최대 1000 페이지까지만 수집할 수 있습니다\")\n",
    "                        break\n",
    "                    else :\n",
    "                        driver.find_element_by_link_text(\"\"\"%s\"\"\" %click_count).click()\n",
    "                   \n",
    "                    current_page += 1\n",
    "                except :\n",
    "                    print(\"총 %s 페이지까지 URL 정보 수집을 완료했습니다\" %click_count)\n",
    "                    break\n",
    "            \n",
    "                time.sleep(random.randrange(1,30))\n",
    "                            \n",
    "\n",
    " \n",
    "\n",
    "       ## 실제 게시글의 내용 추출하기\n",
    "\n",
    "        if cnt > len(article_urls_2) :\n",
    "            cnt = len(article_urls_2)\n",
    "\n",
    "        print(\"전체 검색 결과 건수 :\",len(article_urls_2),\"건\")\n",
    "        print(\"실제 최종 출력 건수\",cnt)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"크롤링 할 총 페이지 번호: \", click_count-1)\n",
    "        print(\"=\" *80)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        print(\"총 %s 건의 데이터를 추출하겠습니다~~~~~~~\" %len(article_urls_2))\n",
    "\n",
    "        no = 1        # 데이터 건수 세기 \n",
    "    \n",
    "        no2=[]           # 목록 번호 저장 변수\n",
    "        write_no2=[]     # 게시글번호 저장 변수\n",
    "        url_2=[]         # 원본 URL 저장 변수\n",
    "        writer_id2=[]    # 작성자 이름 저장 변수\n",
    "        title2=[]        # 제목 저장 변수\n",
    "        wdate2=[]        # 작성 일자 저장 변수\n",
    "        content2=[]      # 내용 저장 변수\n",
    "        reple_cnt2=[]   # 리플 갯수 저장 변수\n",
    "        view_cnt2=[]\n",
    "\n",
    "        # 결과를 화면에 출력하기\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"게시물의 수집을 시작합니다~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\" )\n",
    "        print(\"\\n\")\n",
    "        count = 1\n",
    "    \n",
    "        no=[]           # 목록 번호 저장 변수\n",
    "        write_no=[]     # 게시글 번호 저장 변수\n",
    "        writer_id=[]    # 작성자 이름 저장 변수\n",
    "        title=[]        # 제목 저장 변수\n",
    "        wdate=[]        # 작성 일자 저장 변수\n",
    "        content=[]      # 내용 저장 변수\n",
    "        reple_cnt=[]    # 리플 갯수 저장 변수\n",
    "        view_cnt=[]     # 조회수 저장변수\n",
    "\n",
    "        for article in article_urls_2:\n",
    "       \n",
    "            full_url = cafe_url+'/'+article\n",
    "            driver.get(full_url)\n",
    "        \n",
    "            no2.append(no)            # 1.글번호 컬럼 내용 추가\n",
    "            url_2.append(full_url)    #2. 원본 URL 주소 컬럼 내용 추가\n",
    "        \n",
    "            try :\n",
    "                driver.switch_to.frame('cafe_main')\n",
    "            except  :\n",
    "                continue\n",
    "            else :    \n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "     \n",
    "                f = open(ff_name, 'a',encoding='UTF-8')\n",
    "            \n",
    "                all_content = soup.find('div','inbox')\n",
    "                #print(all_content)\n",
    "            \n",
    "                print(\"%s번째 게시물의 수집을 시작합니다~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\" %count )\n",
    "            \n",
    "                #1.게시글번호 추출 - write_no 변수\n",
    "                w_no_1 = all_content.find('div','etc-box').find('div','fr')\n",
    "                w_no_2 = w_no_1.select('table > tbody > tr > td > span > a')\n",
    "                w_no_3 = w_no_2[0]['href'].split('/')\n",
    "                write_no = w_no_3[4]\n",
    "                write_no2.append(write_no)\n",
    "            \n",
    "                f.write('------------------------------------------------------'+\"\\n\")\n",
    "                f.write(\"%s번째 게시물의 수집을 시작합니다~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\" %count + \"\\n\" )\n",
    "                f.write('1.게시글번호:'+ write_no + \"\\n\")\n",
    "            \n",
    "\n",
    "               #2.게시글에서 작성자 추출 - writer_id 변수\n",
    "                w_name_1 = all_content.find('div','etc-box').find('div','fl')\n",
    "                w_name_2 = w_name_1.select('table > tbody > tr > td > table > tbody > tr > td ')\n",
    "                writer_id = w_name_2[1].get_text()\n",
    "                #print(writer_name)\n",
    "                writer_id2.append(writer_id)\n",
    "                f.write('2.작성자닉네임:'+ writer_id + \"\\n\")\n",
    "            \n",
    "\n",
    "                #3.게시글에서 제목 추출 - title 변수\n",
    "            \n",
    "                title_1 = all_content.find('div','tit-box').find('div','fl')\n",
    "                title_2 = title_1.select('table > tbody > tr > td > span ')\n",
    "                title = title_2[0].get_text().replace(\"\\n\",\"\").strip()\n",
    "                #print(title)\n",
    "                title2.append(title)\n",
    "                f.write('3.게시글제목:'+ title + \"\\n\")\n",
    "                     \n",
    "\n",
    "                #4. 게시글에서 작성일자 추출 - wdate 변수\n",
    "                wdate_1 = all_content.find('div','tit-box').find('div','fr')\n",
    "                wdate_2 = wdate_1.select('table > tbody > tr > td ')\n",
    "                wdate = wdate_2[1].get_text().replace(\"\\n\",\"\").strip()\n",
    "                #print(wdate)\n",
    "                wdate2.append(wdate)\n",
    "                f.write('4.작성일자:'+ wdate + \"\\n\")\n",
    "            \n",
    "\n",
    "                #5. 본문 내용을 출력합니다 - content 변수 사용\n",
    "    \n",
    "                content_1 = all_content.select('#tbody')\n",
    "                content_2 = content_1[0].get_text().replace(\"\\n\",\"\").strip()\n",
    "                print(content_2)\n",
    "                content2.append(content_2)\n",
    "                f.write('5.본문내용:'+ content_2 + \"\\n\")\n",
    "\n",
    "              # 6.게시글에서 댓글수 추출 - reple_cnt 변수사용\n",
    "    \n",
    "                all_reple = soup.select('#cmtMenu')\n",
    "                reple_1 = all_reple[0].select('div > table > tbody > tr > td > a')\n",
    "                reple_2 = reple_1[0].get_text()\n",
    "                reple_3 = reple_2.replace(\",\",\"\")\n",
    "                reple_4 = re.search(\"\\d+\",reple_3)\n",
    "                try :\n",
    "                    reple_cnt = int(reple_4.group())\n",
    "                except :\n",
    "                    reple_cnt = 0\n",
    "                #print(reple_cnt)   \n",
    "                reple_cnt2.append(reple_cnt)\n",
    "                f.write('6.댓글수:'+ str(reple_cnt) + \"\\n\")\n",
    "\n",
    "                # 7.게시글에서 조회수  추출 - view_cnt 변수사용\n",
    "    \n",
    "                view_1 = all_reple[0].select('div > table > tbody > tr')\n",
    "                view_2 = view_1[0].find_all('td')\n",
    "                view_3 = view_2[4].get_text()\n",
    "                view_4 = view_3.replace(\",\",\"\")\n",
    "                view_5 = re.search(\"\\d+\",view_4)\n",
    "                try :\n",
    "                    view_cnt = int(view_5.group())\n",
    "                except :\n",
    "                    view_cnt = 0\n",
    "                #print(view_cnt)   \n",
    "                view_cnt2.append(view_cnt)\n",
    "                f.write('7.조회수:'+ str(view_cnt) + \"\\n\")\n",
    "                time.sleep(random.randrange(1,30)) \n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "                print(\"\\n\")\n",
    "\n",
    "                count += 1\n",
    "                \n",
    "elif choice == 3 :\n",
    "    \n",
    "        now = time.localtime()\n",
    "        s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "        os.makedirs(f_dir+s+'-'+cafe_name)\n",
    "\n",
    "        ff_name=f_dir+s+'-'+cafe_name+'\\\\'+s+'-'+cafe_name+'.txt'\n",
    "        fx_name=f_dir+s+'-'+cafe_name+'\\\\'+s+'-'+cafe_name+'.xls'\n",
    "        fc_name=f_dir+s+'-'+cafe_name+'\\\\'+s+'-'+cafe_name+'.csv'\n",
    "    \n",
    "        g_name = input('조회할 게시글의 주소를 입력하세요:')\n",
    "        driver.get(g_name)\n",
    "        time.sleep(2)\n",
    "    \n",
    "        driver.switch_to.frame('cafe_main')\n",
    "   \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "     \n",
    "        f = open(ff_name, 'a',encoding='UTF-8')\n",
    "    \n",
    "        all_content = soup.find('div','inbox')\n",
    "        #print(all_content)\n",
    "            \n",
    "        print(\"요청하신 게시물의 내용과 댓글 수집을 시작합니다~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\" )\n",
    "        print(\"\\n\")\n",
    "            \n",
    "        #1.게시글번호 추출 - write_no 변수\n",
    "        w_no_1 = all_content.find('div','etc-box').find('div','fr')\n",
    "        w_no_2 = w_no_1.select('table > tbody > tr > td > span > a')\n",
    "        w_no_3 = w_no_2[0]['href'].split('/')\n",
    "        write_no = w_no_3[4]\n",
    "        write_no2.append(write_no)\n",
    "        print('1.게시글번호:',write_no)\n",
    "            \n",
    "        f.write('------------------------------------------------------'+\"\\n\")\n",
    "        f.write(\"%s번째 게시물의 수집을 시작합니다~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\" %count + \"\\n\" )\n",
    "        f.write('1.게시글번호:'+ write_no + \"\\n\")\n",
    "            \n",
    "\n",
    "        #2.게시글에서 작성자 추출 - writer_id 변수\n",
    "        w_name_1 = all_content.find('div','etc-box').find('div','fl')\n",
    "        w_name_2 = w_name_1.select('table > tbody > tr > td > table > tbody > tr > td ')\n",
    "        writer_id = w_name_2[1].get_text()\n",
    "        print('2.작성자닉네임:',writer_id)\n",
    "        writer_id2.append(writer_id)\n",
    "        f.write('2.작성자닉네임:'+ writer_id + \"\\n\")\n",
    "            \n",
    "\n",
    "        #3.게시글에서 제목 추출 - title 변수\n",
    "            \n",
    "        title_1 = all_content.find('div','tit-box').find('div','fl')\n",
    "        title_2 = title_1.select('table > tbody > tr > td > span ')\n",
    "        title = title_2[0].get_text().replace(\"\\n\",\"\").strip()\n",
    "        print('3.게시글제목:',title)\n",
    "        title2.append(title)\n",
    "        f.write('3.게시글제목:'+ title + \"\\n\")\n",
    "                     \n",
    "\n",
    "        #4. 게시글에서 작성일자 추출 - wdate 변수\n",
    "        wdate_1 = all_content.find('div','tit-box').find('div','fr')\n",
    "        wdate_2 = wdate_1.select('table > tbody > tr > td ')\n",
    "        wdate = wdate_2[1].get_text().replace(\"\\n\",\"\").strip()\n",
    "        print('4.작성일자:',wdate)\n",
    "        wdate2.append(wdate)\n",
    "        f.write('4.작성일자:'+ wdate + \"\\n\")\n",
    "            \n",
    "\n",
    "        #5. 본문 내용을 출력합니다 - content 변수 사용\n",
    "    \n",
    "        content_1 = all_content.select('#tbody')\n",
    "        content_2 = content_1[0].get_text().replace(\"\\n\",\"\").strip()\n",
    "        print('5.본문내용:',content_2)\n",
    "        content2.append(content_2)\n",
    "        f.write('5.본문내용:'+ content_2 + \"\\n\")\n",
    "\n",
    "        # 6.게시글에서 댓글수 추출 - reple_cnt 변수사용\n",
    "    \n",
    "        all_reple = soup.select('#cmtMenu')\n",
    "        reple_1 = all_reple[0].select('div > table > tbody > tr > td > a')\n",
    "        reple_2 = reple_1[0].get_text()\n",
    "        reple_3 = reple_2.replace(\",\",\"\")\n",
    "        reple_4 = re.search(\"\\d+\",reple_3)\n",
    "        try :\n",
    "            reple_cnt = int(reple_4.group())\n",
    "        except :\n",
    "            reple_cnt = 0\n",
    "        print('6.댓글수:',reple_cnt)   \n",
    "        reple_cnt2.append(reple_cnt)\n",
    "        f.write('6.댓글수:'+ str(reple_cnt) + \"\\n\")\n",
    "\n",
    "        # 7.게시글에서 조회수  추출 - view_cnt 변수사용\n",
    "    \n",
    "        view_1 = all_reple[0].select('div > table > tbody > tr')\n",
    "        view_2 = view_1[0].find_all('td')\n",
    "        view_3 = view_2[4].get_text()\n",
    "        view_4 = view_3.replace(\",\",\"\")\n",
    "        view_5 = re.search(\"\\d+\",view_4)\n",
    "        try :\n",
    "            view_cnt = int(view_5.group())\n",
    "        except :\n",
    "            view_cnt = 0\n",
    "        print('7.조회수:',view_cnt)   \n",
    "        view_cnt2.append(view_cnt)\n",
    "        f.write('7.조회수:'+ str(view_cnt) + \"\\n\")\n",
    "    \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "        \n",
    "        # 댓글  정보 추출\n",
    "        review2_id=[]\n",
    "        review2_content=[]\n",
    "        review2_date=[]\n",
    "        \n",
    "        all_review= soup.select('#cmt_list > li')\n",
    "        \n",
    "        for b in range(0,len(all_review)) :\n",
    "            try :\n",
    "                review_1 = all_review[b].find('div','comm_cont').find('a','m-tcol-c _rosRestrict _nickUI').get_text().replace(\"작성자\",\"\").strip()\n",
    "            except :\n",
    "                continue\n",
    "                \n",
    "            b += 1\n",
    "            print('-댓글 작성자:', review_1)\n",
    "            review2_id.append(review_1)\n",
    "            \n",
    "            b -= 1\n",
    "            try:\n",
    "                review_2 = all_review[b].find('div','comm_cont').find('p','comm m-tcol-c').get_text().strip()\n",
    "            except :\n",
    "                continue\n",
    "                \n",
    "            b += 1\n",
    "            print('-댓글 내용:', review_2)\n",
    "            review2_content.append(review_2)\n",
    "            \n",
    "            b -= 1\n",
    "            try :\n",
    "                review_3 = all_review[b].find('div','comm_cont').find('span','date m-tcol-c filter-50').get_text().strip()\n",
    "            except :\n",
    "                continue\n",
    "                \n",
    "            b += 1\n",
    "            print('-댓글 작성 날짜:' , review_3)\n",
    "            review2_date.append(review_2)\n",
    "\n",
    "# 결과 데이터프레임화\n",
    "\n",
    "if choice == 1 or choice ==2 :\n",
    "    cafe_df = pd.DataFrame()\n",
    "    cafe_df['게시글번호']=write_no2\n",
    "    cafe_df['게시글 URL']= pd.Series(url_2)\n",
    "    cafe_df['작성자ID']=pd.Series(writer_id2)\n",
    "    cafe_df['게시글제목']=pd.Series(title2)\n",
    "    cafe_df['작성일자']=pd.Series(wdate2)     \n",
    "    cafe_df['본문내용']=pd.Series(content2)\n",
    "    cafe_df['댓글수']=pd.Series(reple_cnt2)\n",
    "    cafe_df['조회수']=pd.Series(view_cnt2)   \n",
    "elif choice == 3 :\n",
    "    cafe_df = pd.DataFrame()\n",
    "    cafe_df['게시글번호']=write_no2\n",
    "    cafe_df['게시글 URL']= pd.Series(url_2)\n",
    "    cafe_df['작성자ID']=pd.Series(writer_id2)\n",
    "    cafe_df['게시글제목']=pd.Series(title2)\n",
    "    cafe_df['작성일자']=pd.Series(wdate2)     \n",
    "    cafe_df['본문내용']=pd.Series(content2)\n",
    "    cafe_df['댓글수']=pd.Series(reple_cnt2)\n",
    "    cafe_df['조회수']=pd.Series(view_cnt2)\n",
    "    cafe_df['댓글작성자'] = pd.Series(review2_id)\n",
    "    cafe_df['댓글내용']= pd.Series(review2_content)\n",
    "    cafe_df['댓글작성일자']=pd.Series(review2_date)\n",
    "    \n",
    "\n",
    "# csv파일로 저장\n",
    "cafe_df.to_csv(fc_name, encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "#엑셀 파일로 저장\n",
    "cafe_df.to_excel(fx_name)\n",
    "\n",
    "count -= 1\n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "print(\"=\" *80)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"txt 파일 저장 완료: 파일명 : %s \" %ff_name)\n",
    "print(\"csv 파일 저장 완료: 파일명 : %s \" %fc_name)\n",
    "print(\"xls 파일 저장 완료: 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *80)\n",
    "\n",
    "# 요약 결과를 txt파일에 저장하기\n",
    "orig_stdout = sys.stdout\n",
    "f = open(f_name, 'a',encoding='UTF-8')\n",
    "sys.stdout = f\n",
    "print(\"=\" *80)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"=\" *80)\n",
    "sys.stdout = orig_stdout\n",
    "f.close()\n",
    "\n",
    "driver.close( )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
