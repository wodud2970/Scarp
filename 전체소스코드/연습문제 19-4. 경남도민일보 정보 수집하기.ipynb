{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "     경남 도민일보의 지역별 뉴스 기사 수집하기     \n",
      "================================================================================\n",
      "\n",
      "    1.전체     2.진주     3.진해     4.통영     5.사천\n",
      "    6.김해     7.밀양     8.거제     9.양산     10.의령\n",
      "    11.함안    12.창녕    13.고성    14.남해    15.하동\n",
      "    16.산청    17.함양    18.거창    19.합천\n",
      "\n",
      "    1.위 지역 중에서 자료를 수집할 분야의 번호를  선택하세요: 2\n",
      "    2.해당 지역에서 크롤링 할 기사 건수는 몇건입니까? : 20\n",
      "    3.파일을 저장할 폴더명만 쓰세요(예:c:\\temp\\):c:\\data\\\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 :  진주 정촌뿌리산단 조성지 또 화석 발견\n",
      "2 :  국방기술품질원 '방위산업 육성' 홍보전시관 개관\n",
      "3 :  경남새마을회 이미용, 수지침 봉사\n",
      "4 :  경남농업기술원, 집중호우 대비 농작물 관리 당부 \n",
      "5 :  경상대 지역 내리사랑 멘토링 운영\n",
      "6 :  세라믹기술원 양파 소비 촉진 릴레이 캠페인\n",
      "7 :  진주시 중기 육성자금 250억 원 늘려\n",
      "8 :  진주 방화·살인 참사 때 헌신한 관리직원 실직 위기\n",
      "9 :  \"서부경남 공공병원 설립 약속 지켜라\"\n",
      "10 :  커지는 진주시의회 생중계 도입 요구\n",
      "11 :  27일 진주시 진양호 활성화 시민설명회\n",
      "12 :  진주시, 6.25전쟁 69주년 행사 개최\n",
      "13 :  남명 조식 마당극으로 만난다\n",
      "14 :  김혜순 시인, 제9회 이형기문학상 수상\n",
      "15 :  진주 방화·살인사건 피해자돕기 모금 마감\n",
      "16 :  법무보호공단 경남서부지소 취업박람회 열어\n",
      "17 :  진주 중앙지하도상가 승강기 공사 중단\n",
      "18 :  진주 동물화장장 갈등 점화…이번엔 대곡면\n",
      "19 :  남동발전 로컬푸드 사업장 이전·개소식\n",
      "20 :  사천 송포동 러브하우스 1호점 탄생\n",
      "20 건  완료========================================================\n",
      "현재까지 총 20 건의 기사를 수집 완료 했습니다\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "총 소요시간은 59.1 초 입니다 \n",
      "파일 저장 완료: txt 파일명 : c:\\data\\경남도민일보-진주-2019-06-29-07-35-40\\경남도민일보-진주-2019-06-29-07-35-40.txt \n",
      "파일 저장 완료: csv 파일명 : c:\\data\\경남도민일보-진주-2019-06-29-07-35-40\\경남도민일보-진주-2019-06-29-07-35-40.csv \n",
      "파일 저장 완료: xls 파일명 : c:\\data\\경남도민일보-진주-2019-06-29-07-35-40\\경남도민일보-진주-2019-06-29-07-35-40.xls \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 경남 도민 일보에서 지역별 뉴스 정보 수집하기\n",
    "\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "#Step 2. 사용자에게 파일이 저장될 폴더명을 입력 받은 후 파일명을 설정합니다.\n",
    "print(\"=\" *80)\n",
    "print(\"     경남 도민일보의 지역별 뉴스 기사 수집하기     \")\n",
    "print(\"=\" *80)\n",
    "\n",
    "query_txt='경남도민일보'\n",
    "query_url='http://www.idomin.com'\n",
    "\n",
    "sec = input('''\n",
    "    1.전체     2.진주     3.진해     4.통영     5.사천\n",
    "    6.김해     7.밀양     8.거제     9.양산     10.의령\n",
    "    11.함안    12.창녕    13.고성    14.남해    15.하동\n",
    "    16.산청    17.함양    18.거창    19.합천\n",
    "\n",
    "    1.위 지역 중에서 자료를 수집할 분야의 번호를  선택하세요: ''')\n",
    "\n",
    "if sec == '' :\n",
    "    sec = 1\n",
    "\n",
    "cnt = int(input('    2.해당 지역에서 크롤링 할 기사 건수는 몇건입니까? : '))\n",
    "if cnt =='' :\n",
    "    cnt = 100\n",
    "    \n",
    "page_cnt = math.ceil(cnt / 20)\n",
    "\n",
    "f_dir = input(\"    3.파일을 저장할 폴더명만 쓰세요(예:c:\\\\temp\\\\):\")\n",
    "if f_dir =='' :\n",
    "     f_dir = \"c:\\\\temp\\\\\"\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "if sec == '1' :\n",
    "    sec_name='전체지역'\n",
    "elif sec =='2' :\n",
    "    sec_name='진주'\n",
    "elif sec =='3' :\n",
    "    sec_name='진해'\n",
    "elif sec =='4' :\n",
    "    sec_name='통영'\n",
    "elif sec =='5' :\n",
    "    sec_name='사천'\n",
    "elif sec =='6' :\n",
    "    sec_name='김해'\n",
    "elif sec =='7' :\n",
    "    sec_name='밀양'\n",
    "elif sec =='8' :\n",
    "    sec_name='거제'\n",
    "elif sec =='9' :\n",
    "    sec_name='양산'\n",
    "elif sec =='10' :\n",
    "    sec_name='의령'\n",
    "elif sec =='11' :\n",
    "    sec_name='함안'\n",
    "elif sec =='12' :\n",
    "    sec_name='창녕'\n",
    "elif sec =='13' :\n",
    "    sec_name='고성'\n",
    "elif sec =='14' :\n",
    "    sec_name='남해'\n",
    "elif sec =='15' :\n",
    "    sec_name='하동'\n",
    "elif sec =='16' :\n",
    "    sec_name='산청'\n",
    "elif sec =='17' :\n",
    "    sec_name='함양'\n",
    "elif sec =='18' :\n",
    "    sec_name='거창'\n",
    "elif sec =='19' :\n",
    "    sec_name='합천'\n",
    " \n",
    "\n",
    "now = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir + query_txt +'-'+sec_name+'-'+s)\n",
    "os.chdir(f_dir + query_txt +'-'+sec_name+'-'+s)\n",
    "\n",
    "ff_name=f_dir + query_txt +'-'+sec_name+'-'+s+'\\\\'+query_txt +'-'+sec_name+'-'+s+'.txt'\n",
    "fc_name=f_dir + query_txt +'-'+sec_name+'-'+s+'\\\\'+query_txt +'-'+sec_name+'-'+s+'.csv'\n",
    "fx_name=f_dir + query_txt +'-'+sec_name+'-'+s+'\\\\'+query_txt +'-'+sec_name+'-'+s+'.xls'\n",
    "\n",
    "#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.\n",
    "\n",
    "s_time = time.time( )\n",
    "\n",
    "path = \"c:/temp/chromedriver_240/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "driver.get(query_url)\n",
    "\n",
    "## Step 4. 지역을 선택합니다\n",
    "url_list=[ ]\n",
    "count = 0\n",
    "\n",
    "driver.find_element_by_xpath('''//*[@id=\"user-nav\"]/button''').click( )\n",
    "time.sleep(1)\n",
    "\n",
    "driver.find_element_by_link_text('''지역''').click( )\n",
    "time.sleep(2)\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "result_1 = soup.find('ul','vertical menu').find('li', class_='is-accordion-submenu-parent')  \n",
    "result_url_list=result_1.find('ul','menu vertical nested submenu is-accordion-submenu is-active').find_all('li')\n",
    "        \n",
    "for i in result_url_list:\n",
    "    url = i.find('a')['href']\n",
    "    #print(url)\n",
    "    url_list.append(url)\n",
    "            \n",
    "\n",
    "# 현재 페이지의 전체 소스코드를 가져와서 각 지역별로 URL 주소를 추출합니다.\n",
    "\n",
    "# 분야별 더보기 버튼을 눌러 페이지를 엽니다\n",
    "if sec == '1' :\n",
    "         driver.get(url_list[0])\n",
    "elif sec == '2' :\n",
    "         driver.get(url_list[1])\n",
    "elif sec == '3' :\n",
    "         driver.get(url_list[2])\n",
    "elif sec == '4' : \n",
    "          driver.get(url_list[3])\n",
    "elif sec == '5' :\n",
    "          driver.get(url_list[4])\n",
    "elif sec == '6' :\n",
    "          driver.get(url_list[5])\n",
    "elif sec == '7' :\n",
    "          driver.get(url_list[6])  \n",
    "elif sec == '8' :\n",
    "          driver.get(url_list[7])\n",
    "elif sec == '9' :\n",
    "          driver.get(url_list[8])\n",
    "elif sec == '10' : \n",
    "          driver.get(url_list[9])\n",
    "elif sec == '11' :\n",
    "          driver.get(url_list[10])\n",
    "elif sec == '12' :\n",
    "          driver.get(url_list[11])\n",
    "elif sec == '13' :\n",
    "          driver.get(url_list[12])\n",
    "elif sec == '14' :\n",
    "          driver.get(url_list[13])\n",
    "elif sec == '15' :\n",
    "          driver.get(url_list[14])\n",
    "elif sec == '16' : \n",
    "          driver.get(url_list[15])\n",
    "elif sec == '17' :\n",
    "          driver.get(url_list[16])\n",
    "elif sec == '18' :\n",
    "          driver.get(url_list[17])\n",
    "elif sec == '19' :\n",
    "          driver.get(url_list[18])\n",
    "\n",
    "#Step 5. 각 지역별 기사의 Title 을 추출합니다.\n",
    "\n",
    "# 스크롤 다운 함수 만들기\n",
    "\n",
    "def scroll_down(driver):\n",
    "    #driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    driver.execute_script(\"window.scrollBy(0,200)\")       \n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "i = 1\n",
    "while (i <= page_cnt):\n",
    "    try:\n",
    "        scroll_down(driver) \n",
    "    except :\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"user-container\"]/div/div[2]/div[2]/section/article/div[2]/div[3]\"\"\").click()\n",
    "    except :\n",
    "        break\n",
    "    #print(\" %s 번째 페이지 정보를 수집합니다\" %i)\n",
    "\n",
    "total_count = 0\n",
    "title2=[]\n",
    "date2=[]\n",
    "\n",
    "try:\n",
    "    html = driver.page_source\n",
    "except:\n",
    "    alert = driver.switch_to.alert\n",
    "    alert.accept()\n",
    "    time.sleep(2)\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "count = 1\n",
    "\n",
    "article_result = soup.find('div', class_='article-list for-custom')\n",
    "ar_list = article_result.find_all('div','items')\n",
    "print(\"\\n\")\n",
    "        \n",
    "f = open(ff_name, 'a',encoding='UTF-8')\n",
    "f.write(\"\\n\")\n",
    "        \n",
    "for li in ar_list:        \n",
    "        title = li.find('strong','titles').get_text()\n",
    "        print(count,\": \",title)\n",
    "        title2.append(title)\n",
    "            \n",
    "        f.write(str(count) + \": \" + title + \"\\n\")        \n",
    "        time.sleep(0.2)\n",
    "            \n",
    "        date = li.find('span','dated').get_text( )\n",
    "        #print(count,\": \",date)\n",
    "        date2.append(date)\n",
    "    \n",
    "        if count < cnt :\n",
    "            count += 1\n",
    "        else :\n",
    "            break\n",
    "    \n",
    "    \n",
    "f.write(\"%s 건  완료=====================================================\" % count + \"\\n\")\n",
    "f.write(\"\\n\")\n",
    "f.close( )\n",
    "                    \n",
    "print(\"%s 건  완료========================================================\" % count)\n",
    "        \n",
    "total_count += count\n",
    "print(\"현재까지 총 %s 건의 기사를 수집 완료 했습니다\" %total_count)\n",
    "print(\"\\n\")\n",
    "time.sleep(1)\n",
    "                                              \n",
    "\n",
    "# Step 6. 출력 결과를 저장하기\n",
    "# 출력 결과를 표(데이터 프레임) 형태로 만들기\n",
    "\n",
    "kndomin = pd.DataFrame()\n",
    "\n",
    "kndomin['기사제목'] = pd.Series(title2) \n",
    "kndomin['보도일자'] = pd.Series(date2)\n",
    "\n",
    "       \n",
    "# csv 형태로 저장하기\n",
    "kndomin.to_csv(fc_name,encoding=\"utf-8-sig\",index=True)\n",
    "\n",
    "# 엑셀 형태로 저장하기\n",
    "kndomin.to_excel(fx_name , index=True)\n",
    "\n",
    "e_time = time.time( )     # 검색이 종료된 시점의 timestamp 를 지정합니다\n",
    "t_time = e_time - s_time\n",
    "\n",
    "#Step 8. 요약정보 출력하기\n",
    "print(\"\\n\") \n",
    "print(\"=\" *80)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"파일 저장 완료: txt 파일명 : %s \" %ff_name)\n",
    "print(\"파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "print(\"파일 저장 완료: xls 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *80)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
